# ğŸ” Multimodal Search Assistant (Text + Image)

This project implements a **multimodal Retrieval-Augmented Generation (RAG)** system combining **text and image search** using vector embeddings and large language models. It supports semantic querying, ranks results across modalities, and presents answers via an interactive web UI.

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ app.py                   # Streamlit frontend
â”œâ”€â”€ scrapper.py              # Scrapes articles from DeepLearning.ai
â”œâ”€â”€ media_downloader.py      # Downloads and stores media locally
â”œâ”€â”€ ingest_data.py           # Embeds and ingests data into Qdrant
â”œâ”€â”€ evaluating.py            # Evaluation scripts (Precision@K, Recall@K)
â”œâ”€â”€ LLM_search.py            # Query handling, retrieval, Gemini integration
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ README.md                # Documentation (this file)
â””â”€â”€ data/
    â”œâ”€â”€ the_batch_articles.csv           # Raw scraped content
    â”œâ”€â”€ articles_with_local_images.csv   # Content with local image paths
    â””â”€â”€ media/                           # Downloaded media files
```

---

## âš™ï¸ Features

- ğŸ” Semantic search across both **text** and **image** data.
- ğŸ¤– Uses **CLIP** and **SentenceTransformer** for vectorization.
- ğŸ“¦ Stores and retrieves vectors using **Qdrant**.
- ğŸ§  Leverages **Gemini 2.0 LLM** to rank and answer queries.
- ğŸ–¼ï¸ Visual frontend built with **Streamlit**.

---

## ğŸš€ Quick Start

### 1. ğŸ”§ Clone the Repository

```bash
git clone https://github.com/your-username/multimodal-search-assistant.git
cd multimodal-search-assistant
```

### 2. ğŸ Create and Activate Virtual Environment

```bash
python -m venv venv
source venv/bin/activate  # or `venv\Scripts\activate` on Windows
```

### 3. ğŸ“¦ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. ğŸ” Set Up Environment Variables

Create a `data.env` file:

```env
GEMINI_API_KEY=your_gemini_api_key_here
```

---


## ğŸ“Š Data Pipeline

> ğŸ§  **Tip:** You can **skip Step 1, Step 2, and Step 3** if you already have a snapshot of the database.
> 
> Simply run Qdrant in Docker and restore the snapshot as shown in the [Backing Up Qdrant](#-backing-up-qdrant) section.
> This will recreate the entire vector database without needing to re-scrape or reprocess data.


### Step 1: Scrape Articles

```bash
python scrapper.py
```

### Step 2: Download Media

```bash
python media_downloader.py
```

### Step 3: Ingest Data into Qdrant

Make sure Qdrant is running locally (e.g., via Docker):

```bash
docker run -p 6333:6333 qdrant/qdrant
```

Then:

```bash
python ingest_data.py
```

---

## ğŸ§ª Evaluation (Optional)

You can evaluate how well the system retrieves relevant content for multiple queries by running the evaluation script.

Run `evaluating.py` and pass a JSON-encoded list of queries as a command-line argument:

Example:


```bash
python evaluating.py --queries '["What is reinforcement learning?", "Recent breakthroughs in AI"]'
```

---

## ğŸ’» Launch the App

```bash
streamlit run app.py
```

Open your browser at `http://localhost:8501`

---

## ğŸ§  Models Used

- **Sentence Transformer**: `intfloat/e5-base`
- **Image Model**: `CLIP ViT-L-14` (OpenCLIP)
- **LLM**: Google Gemini 2.0 Flash
- **Vector DB**: [Qdrant](https://qdrant.tech/)

---

## ğŸ’¾ Backing Up Qdrant

To create a snapshot (backup):

```bash
curl -X POST "http://localhost:6333/collections/articles_collection/snapshots"
```

To restore:

```bash
curl -X POST "http://localhost:6333/collections/articles_collection/snapshots/recover"      -H "Content-Type: application/json"      -d '{"location": "snapshots/articles_collection/your_snapshot_file.tar.gz"}'
```

---

## ğŸ“œ License

MIT License â€” see `LICENSE` file for details.

---

## ğŸ¤ Contributing

Feel free to fork this project and contribute via pull requests.